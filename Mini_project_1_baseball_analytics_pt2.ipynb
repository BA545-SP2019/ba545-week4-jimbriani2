{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project #1: Baseball Analytics\n",
    "\n",
    "The overall purpose of this mini-project is to predicting MLB wins per season by modeling data to KMeans clustering model and linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Further Data Handling\n",
    "\n",
    "In this part, you are going to conduct further data handling tasks to prepare your data for the analysis. The data handling include, but not limited to: \n",
    "- __feature engineering__, \n",
    "- more __data visualization__, \n",
    "- more on __binning your continuous variables__, \n",
    "- __correlation analysis__ and __correlation based feature selection__.\n",
    "\n",
    "Let's get started by importing the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We will need `numpy` in this part - so let's import it as well\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read the data we processed in Part1, and continue working on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../data/baseball_analytics_pt1.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f877c92da033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/baseball_analytics_pt1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../data/baseball_analytics_pt1.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/baseball_analytics_pt1.csv', header=0, index_col=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Anyone who follows the game of baseball knows that, as Major League Baseball (MLB) progressed, different eras emerged where the amount of runs per game increased or decreased significantly. The dead ball era of the early 1900s is an example of a low scoring era and the steroid era at the turn of the 21st century is an example of a high scoring era. Hence, in this analysis, we want to exclude all the game data before the year `1900`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filtering the years before year 1900\n",
    "\n",
    "#### Complete your code below\n",
    "#### You can filter by the value of a certain column `col` in a dataframe `df` as:\n",
    "#### df[df[col] > some_value]\n",
    "#### then you want to save the filtered results as a new dataframe `df`\n",
    "print(df[df[\"year\"] > 1900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are into baseball, you will know Runs per Game (RPG) is an important factor in the game results (winning or not). Since we do not have that feature in the dataset, we are going to create that feaure from the features we have. In the dataset `df`, we have the runs (`df['R']`) and games (`df['G']`) features. We are going to use them to create RPG.\n",
    "\n",
    "Firstly, we are doing the analysis on a yearly basis, so we need to aggregate the runs (`df['R']`) and games (`df['G']`) features to a yearly basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aggregate runs and games data to a yearly basis\n",
    "\n",
    "#### complete the code below\n",
    "#### Aggregation is a very important technique in dataframes\n",
    "#### `pandas` provides a good method called `groupby()` for that\n",
    "#### since we want to aggregate to a yearly basis, we are going to use the feature `yearID`\n",
    "#### and combine the `groupby()` results with `sum()` we will get the aggregated results\n",
    "\n",
    "#### create a new pandas series called `runs_per_year`, \n",
    "#### which is aggregating runs (`df['R']`) on a yearly basis\n",
    "\n",
    "\n",
    "#### do the same for games (`df['G']`)\n",
    "\n",
    "\n",
    "#### We need to combine these two series into a dataframe ('rpg_df') to calculate RPG\n",
    "#### We can do that by using the `pd.concat()` method provided by `pandas`\n",
    "#### Since we want the two series are columns, we need to set the parameter `axis=1` in `concat()`\n",
    "\n",
    "\n",
    "#### Now we can calculate RPG: which is very simple: rpg = runs_per_year/games_per_year\n",
    "\n",
    "\n",
    "#### let's double check the values of `rpg_df` by looking at its first 10 values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Data Visualization\n",
    "\n",
    "Clearly, `rpg_df` is time series data - and in which the most important feature would be `rpg`. So we want to visulize it using line chart. `Matplotlib` has a useful method `plot()` for that purpose. We are going to use that for visualizing `rpg`.\n",
    "\n",
    "First let's import `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "# following statement set a large canvas for the visualization\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Complete the code below\n",
    "#### use `plot()` method on `rpg_df`, and set y-axis as `rpg`.\n",
    "#### the x-axis in this plot will be years, and you should notice that \n",
    "#### 'yearID' is the index of `rpg_df`\n",
    "#### Instead of creating another column of `yearID` (which you can do),\n",
    "#### you can simply tell `plot()` to use the index of the dataframe/series by\n",
    "#### set the `use_index` parameter to `True`\n",
    "\n",
    "\n",
    "#### We want to set the title of the visualization as `MLB Yearly Runs Per Game`\n",
    "\n",
    "\n",
    "#### Set the y-axis as `Runs per Game` use `.xlabel()`\n",
    "\n",
    "\n",
    "#### The x-axis is year - so we set it as `Years`\n",
    "\n",
    "\n",
    "#### show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an alternative way of doing what we just did , which is shown as following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the runs per year and games per year \n",
    "\n",
    "## Use Goupby and sum the count yearly\n",
    "df.groupby(\"yearID\").sum()\n",
    "\n",
    "runs_per_year = {}\n",
    "games_per_year = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    year=row['yearID']\n",
    "    runs=row['R']\n",
    "    games=row['G']\n",
    "    if year in runs_per_year:\n",
    "        runs_per_year[year]=runs_per_year[year]+ runs\n",
    "        games_per_year[year]=games_per_year[year]+ games\n",
    "    else:\n",
    "        runs_per_year[year]=runs\n",
    "        games_per_year[year]=games\n",
    "        \n",
    "## ADD notes to the print statement         \n",
    "print('Runs_per_year', runs_per_year)\n",
    "print(' Games_per_year', games_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these two dictionaries created, we can do the calculation using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mlb runs per game (per year) dictionary \n",
    "runs_per_game = {}\n",
    "for k,v in games_per_year.items():\n",
    "    year = k \n",
    "    games = v\n",
    "    runs = runs_per_year[year]\n",
    "    runs_per_game[year]= runs/games\n",
    "    \n",
    "print(runs_per_game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: dictionaries would be a good alternative of `pandas` functions.\n",
    "\n",
    "Now let's visualize `runs_per_game` again using the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating lists from mlb_runs_per_game\n",
    "## List of tuples (x, y) \n",
    "lists = sorted(runs_per_game.items())\n",
    "x,y = zip(*lists)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title(\"MLB Yearly Runs Per Game\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Runs per Game\")\n",
    "plt.legend('Runs_per_Game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Variable Binning\n",
    "\n",
    "Remember in part 1 we binned wins (`df['W']`) to a categorical variable - that was for making the analysis a __classification__ problem. Sometimes we also using binning for __dimensionality reduction__ purposes. For instance, the feature year (`df['yearID']`) has more than 100 distinct values, so its dimensionality will be 100+. A lot of machine learning algorithms do not like high-dimensional features, so it is a common practice to __bin high-dimensional features__.\n",
    "\n",
    "__Rule of thumb__: if a categorical feature has more than __5__ categories, you should consider bin it.\n",
    "\n",
    "In the block below, you are going to create your own function (`assign_label`) and apply it to your data (`df`).\n",
    "\n",
    "When a categorical variable is low-dimensional, we shoud create dummy variables for that. `pandas` provides a function called `get_dummies()` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the `assign_label` function\n",
    "\n",
    "def assign_label(year):\n",
    "    if year < 1920:\n",
    "        return 1\n",
    "    elif year >= 1920 and year <= 1941:\n",
    "        return 2\n",
    "    elif year >= 1942 and year <= 1945:\n",
    "        return 3\n",
    "    elif year >= 1946 and year <= 1962:\n",
    "        return 4\n",
    "    elif year >= 1963 and year <= 1976:\n",
    "        return 5\n",
    "    elif year >= 1977 and year <= 1992:\n",
    "        return 6\n",
    "    elif year >= 1993 and year <= 2009:\n",
    "        return 7\n",
    "    elif year >= 2010:\n",
    "        return 8\n",
    "\n",
    "#### complete the code below\n",
    "#### Add `year_label` column to `df`\n",
    "#### by applying `assign_label` to the `df['yearID'] column\n",
    "\n",
    "\n",
    "#### create dummy variables for `year_label`\n",
    "#### call `pd.get_dummies` on `df['year_label']`\n",
    "#### set the `prefix` parameter of `get_dummies` to `'era'`\n",
    "#### store the results in a dataframe `dummy_df`\n",
    "\n",
    "\n",
    "#### Concatenate `df` and `dummy_df` using pd.concat()\n",
    "\n",
    "\n",
    "#### Check whether the dummy variables are successfully created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add  *Runs per Game* data from the `runs_per_game` dictionary to `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create column for  runs per game from the runs_per_game dictionary\n",
    "def assign_rpg(year):\n",
    "    return runs_per_game[year]\n",
    "\n",
    "df['rpg'] = df['yearID'].apply(assign_rpg)\n",
    "df['rpg'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way of binning the `yearID` feature is to bin it by **decade**, then create dummy variables based on **dacades**. This can be done using following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert years into decade bins and creating dummy variables\n",
    "def assign_decade(year):\n",
    "    if year < 1920:\n",
    "        return 1910\n",
    "    elif year >= 1920 and year <= 1929:\n",
    "        return 1920\n",
    "    elif year >= 1930 and year <= 1939:\n",
    "        return 1930\n",
    "    elif year >= 1940 and year <= 1949:\n",
    "        return 1940\n",
    "    elif year >= 1950 and year <= 1959:\n",
    "        return 1950\n",
    "    elif year >= 1960 and year <= 1969:\n",
    "        return 1960\n",
    "    elif year >= 1970 and year <= 1979:\n",
    "        return 1970\n",
    "    elif year >= 1980 and year <= 1989:\n",
    "        return 1980\n",
    "    elif year >= 1990 and year <= 1999:\n",
    "        return 1990\n",
    "    elif year >= 2000 and year <= 2009:\n",
    "        return 2000\n",
    "    elif year >= 2010:\n",
    "        return 2010\n",
    "    \n",
    "#### complete the code below\n",
    "#### Add `decade_label` column to `df`\n",
    "#### by applying `assign_decade` function to the `df['yearID'] column\n",
    "\n",
    "\n",
    "#### create dummy variables for `year_label`\n",
    "#### call `pd.get_dummies` on `df['year_label']`\n",
    "#### set the `prefix` parameter of `get_dummies` to `'era'`\n",
    "#### store the results in a dataframe `decade_df`\n",
    "\n",
    "\n",
    "#### Concatenate `df` and `decade_df` using pd.concat()\n",
    "\n",
    "\n",
    "#### Check if above actions are done properly\n",
    "#### by looking at the first 5 rows of `df`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in __feature engineering__ is to remove all unwanted features, in this case, we used `yearID`, `year_level`, and `decade_level` to generate new dummy variables - thus we do not need them any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### complete your code here\n",
    "df = df.drop(####)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Runs per Game_ and _Runs Allowed per Game_ (at __game__ level, not aggregated) are important features in the baseball domain. Now let us go ahead and create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Complete your code below\n",
    "#### runs per game = # of runs/# of games\n",
    "#### # of runs is in `df['R']`\n",
    "#### # of games is in `df['G']`\n",
    "\n",
    "\n",
    "#### runs allowed per game = # of runs allowed/# of games\n",
    "#### # of runs allowed is in `df['RA']\n",
    "\n",
    "\n",
    "#### double check whether our calculations are successful or not\n",
    "#### by looking at the first 10 columns of `df`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.RA_per_game.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "Correlation analysis is an important topic in data analysis, which is also a basis of feature engineering. You want to select features that are __highly__ correlated with your target variable. \n",
    "\n",
    "__Note__: in the meanwhile, if you are using linear models, such as linear regression, decision tree, or naive Bayes, you do not want more than __one__ features are __highly__ correlated.\n",
    "\n",
    "Correlation analysis can be done in several ways: one of the most direct is using the `corr()` method provided in `pandas`. In this part, we are going to calulate the correlation between a particular feature and the target using `numpy`, and then visualize the correlation trends.\n",
    "\n",
    "In above analysis, we believe that **Runs per Game** and **Runs Allowed per Game** are two important features in predicting our target (`win`). We are going to visualize the correlation between each of them and `win` to prove that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a variable called `fig` which contains the size of a figure\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# create two sub-figures, one for `runs per game`, one for `runs allowed per game`\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "# define the axis we use in the figure\n",
    "x1 = df['R_per_game']\n",
    "x2 = df['RA_per_game']\n",
    "y = df['W']\n",
    "\n",
    "#### Complete your code here\n",
    "#### We are using scatterplot to visualize the correlation trend - which is the common practice\n",
    "#### if you don't remember how to create a scatterplot, please refer to part 1\n",
    "#### in the 1st sub-figure (`ax1`), we define the x-axis as 'Runs per Game'\n",
    "#### and y-axis as 'Wins', and color as 'blue'\n",
    "ax1.scatter(x1, y, c ='blue')\n",
    "#### You will also add title as 'Runs per Game vs. Wins'\n",
    "ax1.set_title('Runs per Game vs. Wins')\n",
    "#### and then add proper axis names to it\n",
    "ax1.set_xlabel\n",
    "ax1.set_ylabel\n",
    "# To better visualize the correlation trend, we are going to add trend line to it\n",
    "# Following snippet add trend line of correlation to `ax1`\n",
    "\n",
    "z = np.polyfit(x1, y, 1)\n",
    "p = np.poly1d(z)\n",
    "ax1.plot(x1,p(x1),\"r--\")\n",
    "\n",
    "#### complete your code here\n",
    "#### similarly, we are going to visualize the correlation between `runs allowed per game`\n",
    "#### and `Wins` in `ax2`, we set the color to 'red'\n",
    "plt.scatter(x2, y, c = 'red')\n",
    "#### You will also add title as 'Runs Allowed per Game vs. Wins'\n",
    "plt.set_title('Runs Allowed per Game vs. Wins')\n",
    "#### you will then add x-axis label to `ax2` - note that you do not need \n",
    "#### to add y-axis label since it is shared with `ax1`\n",
    "plt.set_title()\n",
    "\n",
    "# We will use the similar code as above to add trend line in `ax2`\n",
    "z1 = np.polyfit(x2, y, 1)\n",
    "p1 = np.poly1d(z1)\n",
    "ax2.plot(x2,p1(x2),\"g--\")\n",
    "\n",
    "#### show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you observe the different correlation trends in above visualization?\n",
    "\n",
    "To see how each of the variables is correlated with the target variable, we will use the `corr()` method provided by `Pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### complete your code below\n",
    "#### check if features are highly correlated with Wins(`df['W']`) (>.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe some features are __highly__ correlated with Wins.\n",
    "\n",
    "However, that is not the whole picture - for instance, `RA_per_game` is not highly correlated with Wins. In fact, a feature can be __highly__ and __negatively__ correlated with your target.\n",
    "\n",
    "So the correct code should reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### complete your code below\n",
    "#### check if features are highly correlated with Wins(`df['W']`) (>.5 or <-.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can observe a few more are __highly__ correlated with Wins, such as `RA_per_game`.\n",
    "\n",
    "Above code is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = df.corr()['W'] > 0.5 \n",
    "s2 = df.corr()['W'] < -0.5\n",
    "s1 | s2 # logical OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new dataframe `data_features` that only contains useful features (excluding the target and unwanted features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a new dataframe excluding the target variable \n",
    "\n",
    "attributes = ['G','R','AB','H','2B','3B','HR','BB','SO','SB','RA','ER','ERA','CG',\n",
    "             'SHO','SV','IPouts','HA','HRA','BBA','SOA','E','DP','FP','decade_1910','decade_1920',\n",
    "              'decade_1930','decade_1940','decade_1950','decade_1960','decade_1970','decade_1980',\n",
    "              'decade_1990','decade_2000','decade_2010','R_per_game','RA_per_game','rpg']\n",
    "\n",
    "data_features = df[attributes]\n",
    "\n",
    "# check the first 5 rows of `data_features`\n",
    "data_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use following code to determine what columns are excluded in `data_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(df.columns) - set(data_features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe we complete the data preparation phase of the analysis.\n",
    "\n",
    "Let's save the processed data to CSV files so that we can re-use it in part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save all features to a CSV file `baseball_analytics_features.csv`\n",
    "data_features.to_csv('../data/baseball_analytics_features.csv')\n",
    "\n",
    "# don't forget to save the target variable to another CSV file `baseball_analytics_target.csv`\n",
    "df.Win_bins.to_csv('../data/baseball_analytics_target.csv')\n",
    "\n",
    "# we also want to save the # of wins in the original data for further analysis\n",
    "df.W.to_csv('../data/baseball_analytics_wins.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for part 2. Please make sure your sync the complete notebook to your github repo for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
